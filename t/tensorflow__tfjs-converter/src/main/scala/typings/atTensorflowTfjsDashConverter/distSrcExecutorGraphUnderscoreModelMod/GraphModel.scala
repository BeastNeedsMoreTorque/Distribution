package typings.atTensorflowTfjsDashConverter.distSrcExecutorGraphUnderscoreModelMod

import typings.atTensorflowTfjsDashConverter.distSrcDataTypesMod.NamedTensorsMap
import typings.atTensorflowTfjsDashConverter.distSrcDataTypesMod.TensorInfo
import typings.atTensorflowTfjsDashCore.atTensorflowTfjsDashCoreMod.Tensor
import typings.atTensorflowTfjsDashCore.distIoTypesMod.IOHandler
import typings.atTensorflowTfjsDashCore.distIoTypesMod.LoadOptions
import typings.atTensorflowTfjsDashCore.distModelUnderscoreTypesMod.InferenceModel
import typings.atTensorflowTfjsDashCore.distModelUnderscoreTypesMod.ModelPredictConfig
import typings.atTensorflowTfjsDashCore.distTensorUnderscoreTypesMod.NamedTensorMap
import typings.atTensorflowTfjsDashCore.distTypesMod.Rank
import scala.scalajs.js
import scala.scalajs.js.`|`
import scala.scalajs.js.annotation._

@JSImport("@tensorflow/tfjs-converter/dist/src/executor/graph_model", "GraphModel")
@js.native
class GraphModel protected () extends InferenceModel {
  /**
    * @param modelUrl url for the model, or an `io.IOHandler`.
    * @param weightManifestUrl url for the weight file generated by
    * scripts/convert.py script.
    * @param requestOption options for Request, which allows to send credentials
    * and custom headers.
    * @param onProgress Optional, progress callback function, fired periodically
    * before the load is completed.
    */
  def this(modelUrl: String) = this()
  def this(modelUrl: IOHandler) = this()
  def this(modelUrl: String, loadOptions: LoadOptions) = this()
  def this(modelUrl: IOHandler, loadOptions: LoadOptions) = this()
  var convertTensorMapToTensorsMap: js.Any = js.native
  var executor: js.Any = js.native
  var findIOHandler: js.Any = js.native
  var handler: js.Any = js.native
  val inputNodes: js.Array[String] = js.native
  @JSName("inputs")
  val inputs_GraphModel: js.Array[TensorInfo] = js.native
  var loadOptions: js.Any = js.native
  var modelUrl: js.Any = js.native
  val modelVersion: String = js.native
  var normalizeInputs: js.Any = js.native
  var normalizeOutputs: js.Any = js.native
  val outputNodes: js.Array[String] = js.native
  @JSName("outputs")
  val outputs_GraphModel: js.Array[TensorInfo] = js.native
  var version: js.Any = js.native
  val weights: NamedTensorsMap = js.native
  /**
    * Releases the memory used by the weight tensors.
    */
  /** @doc {heading: 'Models', subheading: 'Classes'} */
  def dispose(): Unit = js.native
  def execute(inputs: js.Array[Tensor[Rank]]): Tensor[Rank] | js.Array[Tensor[Rank]] = js.native
  /**
    * Executes inference for the model for given input tensors.
    * @param inputs tensor, tensor array or tensor map of the inputs for the
    * model, keyed by the input node names.
    * @param outputs output node name from the Tensorflow model, if no
    * outputs are specified, the default outputs of the model would be used.
    * You can inspect intermediate nodes of the model by adding them to the
    * outputs array.
    *
    * @returns A single tensor if provided with a single output or no outputs
    * are provided and there is only one default output, otherwise return a
    * tensor array. The order of the tensor array is the same as the outputs
    * if provided, otherwise the order of outputNodes attribute of the model.
    */
  /** @doc {heading: 'Models', subheading: 'Classes'} */
  def execute(inputs: Tensor[Rank]): Tensor[Rank] | js.Array[Tensor[Rank]] = js.native
  def execute(inputs: Tensor[Rank], outputs: String): Tensor[Rank] | js.Array[Tensor[Rank]] = js.native
  def execute(inputs: Tensor[Rank], outputs: js.Array[String]): Tensor[Rank] | js.Array[Tensor[Rank]] = js.native
  def execute(inputs: NamedTensorMap): Tensor[Rank] | js.Array[Tensor[Rank]] = js.native
  def executeAsync(inputs: js.Array[Tensor[Rank]]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
  def executeAsync(inputs: js.Array[Tensor[Rank]], outputs: String): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
  def executeAsync(inputs: js.Array[Tensor[Rank]], outputs: js.Array[String]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
  /**
    * Executes inference for the model for given input tensors in async
    * fashion, use this method when your model contains control flow ops.
    * @param inputs tensor, tensor array or tensor map of the inputs for the
    * model, keyed by the input node names.
    * @param outputs output node name from the Tensorflow model, if no outputs
    * are specified, the default outputs of the model would be used. You can
    * inspect intermediate nodes of the model by adding them to the outputs
    * array.
    *
    * @returns A Promise of single tensor if provided with a single output or
    * no outputs are provided and there is only one default output, otherwise
    * return a tensor map.
    */
  /** @doc {heading: 'Models', subheading: 'Classes'} */
  def executeAsync(inputs: Tensor[Rank]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
  def executeAsync(inputs: Tensor[Rank], outputs: String): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
  def executeAsync(inputs: Tensor[Rank], outputs: js.Array[String]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
  def executeAsync(inputs: NamedTensorMap): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
  def executeAsync(inputs: NamedTensorMap, outputs: String): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
  def executeAsync(inputs: NamedTensorMap, outputs: js.Array[String]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
  /**
    * There's no native PRelu op in TensorFlow, so Keras generates the following
    * structure which does the equivalent calculation:
    * f(x) = Relu(x) + (-alpha * Relu(-x))
    * Since tfjs-core has a prelu op, this method will fuse the TensorFlow
    * generated ops into prelu op. It will also try to register a custom op that
    * supports prelu op.
    */
  def fusePrelu(): Unit = js.native
  /**
    * Loads the model and weight files, construct the in memory weight map and
    * compile the inference graph.
    */
  def load(): js.Promise[Boolean] = js.native
  def predict(inputs: js.Array[Tensor[Rank]]): Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap = js.native
  /**
    * Execute the inference for the input tensors.
    *
    * @param input The input tensors, when there is single input for the model,
    * inputs param should be a `tf.Tensor`. For models with mutliple inputs,
    * inputs params should be in either `tf.Tensor`[] if the input order is
    * fixed, or otherwise NamedTensorMap format.
    *
    * For model with multiple inputs, we recommend you use NamedTensorMap as the
    * input type, if you use `tf.Tensor`[], the order of the array needs to
    * follow the
    * order of inputNodes array. @see {@link GraphModel.inputNodes}
    *
    * You can also feed any intermediate nodes using the NamedTensorMap as the
    * input type. For example, given the graph
    *    InputNode => Intermediate => OutputNode,
    * you can execute the subgraph Intermediate => OutputNode by calling
    *    model.execute('IntermediateNode' : tf.tensor(...));
    *
    * This is useful for models that uses tf.dynamic_rnn, where the intermediate
    * state needs to be fed manually.
    *
    * For batch inference execution, the tensors for each input need to be
    * concatenated together. For example with mobilenet, the required input shape
    * is [1, 244, 244, 3], which represents the [batch, height, width, channel].
    * If we are provide a batched data of 100 images, the input tensor should be
    * in the shape of [100, 244, 244, 3].
    *
    * @param config Prediction configuration for specifying the batch size and
    * output node names. Currently the batch size option is ignored for graph
    * model.
    *
    * @returns Inference result tensors. The output would be single `tf.Tensor`
    * if model has single output node, otherwise Tensor[] or NamedTensorMap[]
    * will be returned for model with multiple outputs.
    */
  /** @doc {heading: 'Models', subheading: 'Classes'} */
  def predict(inputs: Tensor[Rank]): Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap = js.native
  def predict(inputs: Tensor[Rank], config: ModelPredictConfig): Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap = js.native
  def predict(inputs: NamedTensorMap): Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap = js.native
}

